---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Post-Doctor at the Huazhong University of Science and Technology (华中科技大学), in the [HUST Media Lab (智能媒体计算与网络安全实验室)](http://media.hust.edu.cn){:style="text-decoration: none"}, under the supervision of Prof. [Junqing Yu (于俊清)](http://faculty.hust.edu.cn/yujunqing/zh_CN/more/370059/jsjjgd/index.htm){:style="text-decoration: none"}. I am currently as an visiting scholar at the National University of Singapore (NUS), under the supervision of Prof. [Xinchao Wang](https://sites.google.com/site/sitexinchaowang){:style="text-decoration: none"}.
I major in Computer Science and my research interests lie in the areas of **computer vision**, **multimedia models**, **motion estimation**, and **social media analysis**.

News
---
<div style="height: 200px; overflow: auto">
  <ul>
    <li><strong><font color="red">2025.3</font></strong>: Our paper on medical learning is accepted to <strong><em>ICME'25</em></strong>, congrating to Qilong.</li>
    <li><strong><font color="red">2025.2</font></strong>: Our Two papers on VLLM/3D are accepted to <strong><em>CVPR'25</em></strong>, congrating to Yangliu and Youjia.</li>
    <li><strong><font color="red">2025.2</font></strong>: Our paper on MLLM is accepted to <strong><em>ICLR'25 Spotlight</em></strong>, congrating to Luo Run.</li>
    <li><strong><font color="red">2024.12</font></strong>: Our Two papers on Tracking/Anomaly Detection are accepted to <strong><em>AAAI'25</em></strong>, congrating to Zhou Hang.</li>
    <li><strong><font color="red">2024.9</font></strong>: Our paper on Multimodal Tech is accepted to <strong><em>NeurIPS'24</em></strong>, congrating to Wenbing.</li>
    <li><strong><font color="red">2024.7</font></strong>: Our paper on Point Tracking is accepted to <strong><em>ACM MM'24</em></strong>.</li>
    <li><strong><font color="red">2024.3</font></strong>: Our paper on Feature Compress is accepted to <strong><em>ICME'24</em></strong>, congrating to Tang Ying</li>
    <li><strong><font color="red">2023.12</font></strong>: Our paper on Tracking is accepted to <strong><em>AAAI'24</em></strong>, congrating to Luo Run.</li>
    <li><strong><font color="red">2022.12</font></strong>: Our paper on Tracking is accepted to <strong><em>AAAI'23</em></strong>.</li>
    <li><strong><font color="red">2022.2</font></strong>: Our paper on Tracking is accepted to <strong><em>CVPR'22</em></strong>.</li>
    <li><strong><font color="red">2021.8</font></strong>: Our paper on Tracking is accepted to <strong><em>ICMR'21</em></strong>.</li>
  </ul>
</div>

Education
---
* B.S. in University of Electronic Science and Technology of China (电子科技大学), 2012~2016
* M.S. in Huazhong University of Science and Technology (华中科技大学), 2016~2019
* Ph.D in Huazhong University of Science and Technology (华中科技大学), 2019~2023

Award and Service
---
* ACM Outstanding Student
* Outstanding Doctoral Scholarship
* Conference PC/Reviewers : CVPR23/24/25, ICCV23/25, ECCV24, AAAI24/25, NIPS24/25, ICLR25, ACM MM24/25, IJCAI24
* Journal Reviewers : TIP, PR, TCSVT, TMM, KBS, SIGPRO

Publications
---
<table style="border-collapse:collapse; border:none">
<tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/ICME25.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="">CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation</a>
          <br>
          Qilong Xing, <strong>Zikai Song*</strong>, Yuteng Ye, Yuke Chen, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang*
          <br>
          <em>ICME</em>,2025
          <br>
          <a href="">paper</a>
          /
          <a href="">code</a>
          <p></p>
          <p>
            We propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating spatial anatomical features to enhance segmentation accuracy of the diffusion model.
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/CVPR25_1.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="">SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding</a>
          <br>
          Yangliu Hu, <strong>Zikai Song*</strong>, Na Feng, Yawei Luo, Junqing Yu, Yi-Ping Phoebe Chen, Wei Yang*
          <br>
          <em>CVPR</em>,2025
          <br>
          <a href="">paper</a>
          /
          <a href="">code</a>
          <p></p>
          <p>
            We propose a Self-Supervised Fragment FineTuning (SF2T), a novel effortless fine-tuning method, employs the rich inherent characteristics of videos for training, while unlocking more fine-grained understanding ability of Video-LLMs. 
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/CVPR25_2.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="">Ref-GS: Directional Factorization for 2D Gaussian Splatting</a>
          <br>
          Youjia Zhang, Anpei Chen, Yumin Wan, <strong>Zikai Song</strong>, Junqing Yu, Yawei Luo, Wei Yang*
          <br>
          <em>CVPR</em>,2025
          <br>
          <a href="https://arxiv.org/pdf/2412.00905">paper</a>
          /
          <a href="">code</a>
          <p></p>
          <p>
            We introduce Ref-GS, a novel approach for directional light factorization in 2D Gaussian splatting, which enables photorealistic view-dependent appearance rendering and precise geometry recovery. 
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/ICLR25.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="">Deem: Diffusion models serve as the eyes of large language models for image perception</a>
          <br>
          Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, <strong>Zikai Song</strong>, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui
          <br>
          <em>ICLR</em>,2025
          <font color="tomato">
            <strong>(Spotlight)</strong>
          </font>
          <br>
          <a href="https://arxiv.org/pdf/2405.15232">paper</a>
          /
          <a href="">code</a>
          <p></p>
          <p>
            We propose DEEM, a simple but effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. 
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/OFTrack.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="">Temporal Coherent Object Flow for Multi-Object Tracking</a>
          <br>
          <strong>Zikai Song</strong>, Run Luo, Lintao Ma, Ying Tang, Yi-Ping Phoebe Chen, Junqing Yu, Wei Yang*
          <br>
          <em>AAAI</em>,2025
          <br>
          <a href="">paper</a>
          /
          <a href="">code</a>
          <p></p>
          <p>
            We propose a section-based multi-object tracking approach that integrates a temporal coherent Object Flow Tracker, capable of achieving simultaneous multi-frame tracking by treating multiple consecutive frames as the basic processing unit, denoted as a “section”.
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/zhouhangaaai24.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="">Video Anomaly Detection with Motion and Appearance Guided Patch Diffusion Model</a>
          <br>
          Hang Zhou, Cai Jiale, Yuteng Ye, Yonghui Feng, Chenxing Gao, Junqing Yu, <strong>Zikai Song*</strong>, Wei Yang
          <br>
          <em>AAAI</em>,2025
          <br>
          <a href="">paper</a>
          /
          <a href="">code</a>
          <p></p>
          <p>
            We introduce innovative motion and appearance conditions that are seamlessly integrated into our patch diffusion model.
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/couplemamba.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://arxiv.org/pdf/2405.18014">Coupled Mamba: Enhanced Multimodal Fusion with Coupled State Space Model</a>
          <br>
          Wenbing Li, Hang Zhou, Junqing Yu, <strong>Zikai Song*</strong>, Wei Yang*
          <br>
          <em>NeurIPS</em>,2024
          <br>
          <a href="https://arxiv.org/pdf/2405.18014">paper</a>
          /
          <a href="https://github.com/hustcselwb/coupled-mamba">code</a>
          <p></p>
          <p>
            We propose the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes.
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/ALTrack.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://www.arxiv.org/abs/2407.20730">Autogenic Language Embedding for Coherent Point Tracking</a>
          <br>
          <strong>Zikai Song</strong>, Ying Tang, Run Luo, Lintao Ma, Junqing Yu, Yi-Ping Phoebe Chen, Wei Yang*
          <br>
          <em>ACM MM</em>,2024
          <br>
          <a href="https://www.arxiv.org/abs/2407.20730">paper</a>
          <p></p>
          <p>
            We introduce a novel approach leveraging language embeddings to enhance the coherence of frame-wise visual features related to the same object.
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/ICME24.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://ieeexplore.ieee.org/abstract/document/10687920">Agnostic Feature Compression with Semantic Guided Channel Importance Analysis</a>
          <br>
          Ying Tang, Wei Yang, Junqing Yu, <strong>Zikai Song*</strong>
          <br>
          <em>ICME</em>,2024
          <br>
          <a href="https://ieeexplore.ieee.org/abstract/document/10687920">paper</a>
          <p></p>
          <p>
            We can apply compression operation to a deeper degree for less irrelevant parts to achieve a high compression rate, while preserving the performance by applying a lower compression ratio to the more important parts.
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/amd.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://arxiv.org/abs/2312.12763">AMD: Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion</a>
          <br>
          Beibei Jing, Youjia Zhang, <strong>Zikai Song</strong>, Junqing Yu, Wei Yang*
          <br>
          <em>AAAI</em>,2024
          <br>
          <a href="https://arxiv.org/abs/2312.12763">arXiv</a>
          <p></p>
          <p>
            We propose the Adaptable Motion Diffusion (AMD) model, which leverages a Large Language Model (LLM) to parse the input text into a sequence of concise and interpretable anatomical scripts that correspond to the target motion.
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/pt2i.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://arxiv.org/abs/2309.09466">Progressive Text-to-Image Diffusion with Soft Latent Direction</a>
          <br>
          Yuteng Ye, Jiale Cai, Hang Zhou, Guanwen Li, Youjia Zhang, <strong>Zikai Song</strong>, Chenxing Gao, Junqing Yu, Wei Yang*
          <br>
          <em>AAAI</em>,2024
          <br>
          <a href="https://arxiv.org/abs/2309.09466">arXiv</a>
          /
          <a href="https://github.com/babahui/Progressive-Text-to-Image">code</a>
          <p></p>
          <p>
            We propose to harness the capabilities of a Large Language Model (LLM) to decompose text descriptions into coherent directives adhering to stringent formats and progressively generate the target image.
          </p>
        </td>
    </tr>
  <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/difftrack.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://arxiv.org/abs/2308.09905">DiffusionTrack: Diffusion Model For Multi-Object Tracking</a>
          <br>
          Run Luo, <strong>Zikai Song*</strong>, Lintao Ma, Jinlin Wei, Wei Yang, Min Yang*
          <br>
          <em>AAAI</em>,2024
          <br>
          <a href="https://arxiv.org/abs/2308.09905">arXiv</a>
          /
          <a href="https://github.com/RainBowLuoCS/DiffusionTrack">code</a>
          <p></p>
          <p>
            We formulates object detection and association jointly as a consistent denoising diffusion process from paired noise boxes to paired ground-truth boxes.
          </p>
        </td>
    </tr>
    <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/cttrack.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://arxiv.org/abs/2301.10938">Compact Transformer Tracker with Correlative Masked Modeling</a>
          <br>
          <strong>Zikai Song</strong>, Run Luo, Junqing Yu*, Yi-Ping Phoebe Chen, Wei Yang*
          <br>
          <em>AAAI</em>,2023
          <font color="tomato">
            <strong>(Oral Presentation)</strong>
          </font>
          <br>
          <a href="https://arxiv.org/abs/2301.10938">arXiv</a>
          /
          <a href="https://github.com/HUSTDML/CTTrack">code</a>
          <p></p>
          <p>
            We demonstrate the basic vision transformer (ViT) architecture is sufficient for visual tracking with correlative masked modeling for information aggregation enhancement.
          </p>
        </td>
    </tr>
    <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/cswintt.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://arxiv.org/abs/2205.03806">Transformer Tracking with Cyclic Shifting Window Attention</a>
          <br>
          <strong>Zikai Song</strong>, Junqing Yu*, Yi-Ping Phoebe Chen, Wei Yang
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2205.03806">arXiv</a>
          /
          <a href="https://github.com/SkyeSong38/CSWinTT">code</a>
          <p></p>
          <p>
            CSWinTT is a new transformer architecture with multi-scale cyclic shifting window attention for visual object tracking, elevating the attention from pixel to window level.
          </p>
        </td>
    </tr>
   <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/ICMR21.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://dl.acm.org/doi/abs/10.1145/3460426.3463629">Distractor-Aware Tracker with a Domain-Special Optimized Benchmark for Soccer Player Tracking</a>
          <br>
          <strong>Zikai Song</strong>, Zhiwen Wan, Wei Yuan, Ying Tang, Junqing Yu, Yi-Ping Phoebe Chen
          <br>
          <em>ICMR</em>, 2021
          <br>
          <a href="http://media.hust.edu.cn/dataset.htm">Project Page</a>
          /
          <a href="https://dl.acm.org/doi/abs/10.1145/3460426.3463629">paper</a>
          <p></p>
          <p>
            We proposed a distractor-aware player tracking algorithm and a high-quality benchmark for soccer play tracking, deal with occlusion and similar distractors in soccer scenes.
          </p>
        </td>
    </tr>
   <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/MMM20.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://dl.acm.org/doi/abs/10.1145/3460426.3463629">Fine-Grain Level Sports Video Search Engine</a>
          <br>
          <strong>Zikai Song</strong>, Junqing Yu, Hengyou Cai, Yangliu Hu, Yi-Ping Phoebe Chen
          <br>
          <em>MultiMedia Modeling</em>, 2020
          <br>
          <a href="https://link.springer.com/chapter/10.1007/978-3-030-37731-1_42">paper</a>
          <p></p>
          <p>
            We designed and developed a sports video search engine based on distributed architecture, aimimng to provide content-based video analysis and retrieval services
          </p>
        </td>
    </tr>
   <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/SSET.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://www.researchgate.net/profile/Zikai-Song-2/publication/343520973_SSET_a_dataset_for_shot_segmentation_event_detection_player_tracking_in_soccer_videos/links/627778df2f9ccf58eb3703ec/SSET-a-dataset-for-shot-segmentation-event-detection-player-tracking-in-soccer-videos.pdf">SSET: a dataset for shot segmentation, event detection, player tracking in soccer videos</a>
          <br>
          Na Feng, <strong>Zikai Song</strong>, Junqing Yu, Yi-Ping Phoebe Chen, Yizhu Zhao, Yunfeng He, Tao Guan
          <br>
          <em>Multimedia Tools and Applications</em>, 2020
          <br>
          <a href="http://media.hust.edu.cn/dataset.htm">Project Page</a>
          /
          <a href="https://www.researchgate.net/profile/Zikai-Song-2/publication/343520973_SSET_a_dataset_for_shot_segmentation_event_detection_player_tracking_in_soccer_videos/links/627778df2f9ccf58eb3703ec/SSET-a-dataset-for-shot-segmentation-event-detection-player-tracking-in-soccer-videos.pdf">Paper</a>
          <p></p>
          <p>
            We construct a soccer dataset named Soccer Dataset for Shot, Event, and Tracking, to meet the research needs of shot segmentation, event detection and player tracking
          </p>
        </td>
    </tr>
   <tr>
        <td varlign="middle" width="25%" style="border:none"> <img src="https://skyesong38.github.io/images/MIPR.png" /> </td>
        <td varlign="middle" width="75%" style="border:none">
          <a href="https://www.researchgate.net/profile/Zikai-Song-2/publication/326047906_Comprehensive_Dataset_of_Broadcast_Soccer_Videos/links/627779b1b1ad9f66c8ab509d/Comprehensive-Dataset-of-Broadcast-Soccer-Videos.pdf">Comprehensive dataset of broadcast soccer videos</a>
          <br>
          Junqing Yu, Aiping Lei, <strong>Zikai Song</strong>, Tingting Wang, Hengyou Cai, Na Feng
          <br>
          <em>MIPR</em>, 2018
          <br>
          <a href="http://media.hust.edu.cn/dataset.htm">project</a>
          /
          <a href="https://www.researchgate.net/profile/Zikai-Song-2/publication/326047906_Comprehensive_Dataset_of_Broadcast_Soccer_Videos/links/627779b1b1ad9f66c8ab509d/Comprehensive-Dataset-of-Broadcast-Soccer-Videos.pdf">paper</a>
          <p></p>
          <p>
            We focus on broadcast soccer videos and present a comprehensive dataset for analysis, including shot boundaries, event annotations, and bounding boxes.
          </p>
        </td>
    </tr>
</table>

Project
---
* 2023 入选国家资助博士后计划
* 2024 主持中国博士后科学基金面上项目
* 2024 主持国家自然青年科学基金青年基金项目
* 2024 主持湖北省博新计划项目
